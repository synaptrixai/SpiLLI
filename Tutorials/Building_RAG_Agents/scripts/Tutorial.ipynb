{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e300a830-db9d-4cd2-84a6-57b06a7afaae",
   "metadata": {},
   "source": [
    "# Step 1: Retrieving Information from different sources (Websites, Files, Databases ...)\n",
    "\n",
    "## 1A: Loading Web Content \n",
    "\n",
    "We'll use LangChain's Selenium plugin from the Unstructured library to retrieve content from websites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c57acf0-afe0-4f44-83f3-eda47e7a8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import SeleniumURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22371cc-6ea7-4233-9d18-1c8bcad38106",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://raw.githubusercontent.com/synaptrixai/SpiLLI/refs/heads/main/README.md\"]\n",
    "loader = SeleniumURLLoader(urls=urls, browser='chrome', headless=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91df3ee8-eac3-42b2-a93d-484059f32019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SpiLLI  \n",
      "*Decentralized AI inference infrastructure*  \n",
      "\n",
      "\n",
      "SpiLLI provides infrastructure to manage, host, deploy and run decentralized AI inference\n",
      "\n",
      "SpiLLI consists of two main components:\n",
      "\n",
      "| Component | Purpose |\n",
      "|-----------|---------|\n",
      "| **SpiLLI SDK** | Python library/framework for building decentralized AI applications |\n",
      "| **SpiLLIHost** | Sandboxed AI Runner that turns a machine into a host node, serving AI models to the network |\n",
      "\n",
      "> **‚ö†Ô∏è Notice** ‚Äì SpiLLI is in **beta**. It may contain bugs and features under development.  \n",
      "> Kindly open issues, give feedback, or contribute on GitHub.\n",
      "\n",
      "---\n",
      "\n",
      "## How SpiLLI Works  \n",
      "\n",
      "SpiLLI is a global, peer‚Äëto‚Äëpeer network of AI hosts and users.  \n",
      "* A machine running **SpiLLIHost** becomes a *host node*.  \n",
      "* Apps built with **SpiLLI SDK** can discover and use models on any host node.  \n",
      "* Connections are made without a central cloud provider.\n",
      "\n",
      "The network is built around three principles:\n",
      "\n",
      "1. **Dynamic Resource Allocation** ‚Äì Apps find the best host based on load, performance, and proximity.  \n",
      "2. **Decentralized Infrastructure** ‚Äì Computing power is spread across many nodes, providing redundancy.  \n",
      "3. **Real‚Äëtime Adaptation** ‚Äì Connections and allocations are continuously optimized.\n",
      "\n",
      "This reduces costs and increases accessibility for developers, users and AI researchers.\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "### 1Ô∏è‚É£ Running Applications and Tutorials in a pre-configured, sandboxed environment\n",
      "\n",
      "The repository ships a [Docker image](https://hub.docker.com/r/synaptrixai/spilli-rag-tutorials) with all required dependencies installed for easy startup. If you have [Docker](https://docs.docker.com/get-started/introduction/get-docker-desktop/) installed, you can run the tutorials using the following steps:\n",
      "\n",
      "| Step | Command |\n",
      "|------|---------|\n",
      "| Clone the repo | `git clone https://github.com/synaptrixai/SpiLLI.git` |\n",
      "| Download your PEM encryption | Download a personalized encryption file from [SpiLLI Demo](https://agents.synaptrix.org/dechat) Put the downloaded `.pem` file next to the `docker-compose.yml` file in the cloned repository |\n",
      "| Start Docker | `docker compose up` (or sometimes `docker-compose up` on windows) |\n",
      "| Access Jupyter | Open <http://127.0.0.1:8888/lab> to access and run the tutorials |\n",
      "\n",
      "**Sample tutorials**\n",
      "\n",
      "| Tutorial | Link |\n",
      "|----------|------|\n",
      "| Building RAG agents | <http://127.0.0.1:8888/lab/tree/Tutorials/Building_RAG_Agents/scripts/Tutorial.ipynb> |\n",
      "| Building Tools for AI agents | <http://127.0.0.1:8888/lab/tree/Tutorials/Building_Agent_Tools/Tutorial.ipynb> |\n",
      "\n",
      "> **Tip** ‚Äì The `Tutorials` folder is volume‚Äëmapped into the container.  \n",
      "> Edit or add notebooks on your host; changes appear instantly inside Jupyter and vise-versa.\n",
      "\n",
      "> **Contributing** ‚Äì Create a branch, commit your notebooks, and open a PR (Pull Request).  \n",
      "> Discuss major changes in the *Discussions* or *Ideas* tab first.\n",
      "\n",
      "---\n",
      "\n",
      "### 2Ô∏è‚É£ System Requirements\n",
      "\n",
      "| OS | Host | SDK |\n",
      "|----|------|-----|\n",
      "| Ubuntu 24.04 | ‚úÖ | Python 3.8‚Äì3.12 |\n",
      "| Windows 10/11 | ‚úÖ | Python 3.8‚Äì3.12 |\n",
      "| Others | üöß (future) | üöß |\n",
      "\n",
      "> **Host**: Supports NVIDIA, AMD GPUs and CPUs.  \n",
      "> **SDK**: Use the Python package manager to install the SDK.\n",
      "\n",
      "---\n",
      "\n",
      "### 3Ô∏è‚É£ Installation Without Docker\n",
      "\n",
      "#### Installing SpiLLIHost\n",
      "\n",
      "- **Ubuntu**  \n",
      "  1. Download the `.deb`: <https://github.com/synaptrixai/SpiLLI/releases/download/v0.3.2/SpiLLIHost-0.3.2-Linux-SpiLLIHost.deb>\n",
      "  2. Install:  \n",
      "     ```bash\n",
      "     sudo apt install ./SpiLLIHost-0.3.2-Linux-SpiLLIHost.deb\n",
      "     ```  \n",
      "  3. Move the PEM file to the host directory:  \n",
      "     ```bash\n",
      "     sudo mv SpiLLIHost_Community.pem /usr/bin/SpiLLIHost/\n",
      "     ```  \n",
      "\n",
      "- **Windows**  \n",
      "  1. Download the installer: <https://github.com/synaptrixai/SpiLLI/releases/download/v0.3.2/SpiLLIHost-0.3.2-win64.exe>  \n",
      "  2. Run the setup. Ignore the unsigned‚Äësignature warning.  \n",
      "  3. Select your `SpiLLIHost_Community.pem` during the installation process and follow prompts to complete the installation.\n",
      "\n",
      "After the installation is complete, SpiLLIHost is automatically started as a service on windows/ubuntu and nothing further needs to be done.\n",
      "\n",
      "To manage the **Service**  \n",
      "   - **Windows**: Use the Windows *Services* console [can be found by typing \"Services\" in the windows start search bar]. Double click the \n",
      "   SpiLLIHost service in the Windows Services UI to open a dialog to start/stop/restart/or check the service status.  \n",
      "   - **Ubuntu**: Use systemctl in the terminal as admin. `sudo systemctl start [ other options: stop/restart/status] SpiLLIHost.service`.  \n",
      "\n",
      "> **Troubleshooting** The SpiLLIHost service will not run if there is no encryption file provided. If the service fails to start check if your downloaded pem file is stored in the installed directory (\"/usr/bin/SpiLLIHost\" for Ubuntu, \"C:\\Program Files (x86)\\SpiLLIHost\\bin\" by default on Windows). The pem file should be named as  SpiLLIHost_Community.pem\n",
      "\n",
      "> **‚ö†Ô∏è Security** The pem file you download provides personalized client and host side encryption ensuring that the data in transit over the network is only accessible by you. So do not share the file publicly or commit it to the repository.\n",
      "\n",
      "#### Installing SpiLLI SDK\n",
      "\n",
      "Use the python pip installer in the terminal using\n",
      "```bash\n",
      "pip install --index-url https://well.synaptrix.org --upgrade SpiLLI\n",
      "```\n",
      "\n",
      "> Get the `SpiLLIHost_Community.pem` and `SpiLLI_Community.pem` encryption files from <https://agents.synaptrix.org/dechat>.\n",
      "\n",
      "---\n",
      "\n",
      "## How to Use \n",
      "\n",
      "| Role | What to do |\n",
      "|------|------------|\n",
      "| **AI Host** | Install SpiLLIHost, join the network, manage hosted models via the UI at <https://agents.synaptrix.org/dechat>. |\n",
      "| **Developer** | Build apps using the SDK. Get featured in  the repo by contributing tutorials, example applications and submit PRs. |\n",
      "| **Feedback** | Open issues, suggest features, share ideas using the corresponding tabs on [GitHub](https://github.com/synaptrixai/SpiLLI). |\n",
      "| **Sponsor** | Contact us to organize events like hackathons or AI agent tutorials for your community at `community@synaptrix.org`. |\n",
      "\n",
      "> Star the repo if you like what we‚Äôre building!  \n",
      "\n",
      "---\n",
      "\n",
      "## Running as an AI Host\n",
      "\n",
      "1. **Installation** ‚Äì Follow the instructions above.  \n",
      "2. **Service Management**  \n",
      "   - **Windows**: Use the *Services* console.  \n",
      "   - **Ubuntu**: `systemctl start spilli-host.service` / `stop`.  \n",
      "3. **Hosting Models**  \n",
      "   - Via the portal: <https://agents.synaptrix.org/dechat> ‚Üí *Manage Hosted Models*.  \n",
      "   - Or drop `.gguf` files into the host directory (`/usr/bin/SpiLLIHost/Models` on Ubuntu, `C:\\Program Files (x86)\\SpiLLIHost\\bin\\Models` on Windows).\n",
      "\n",
      "---\n",
      "\n",
      "## Running as an AI Developer\n",
      "\n",
      "* Explore the **Tutorials** folder for example code.  \n",
      "* Follow us on [LinkedIn](https://www.linkedin.com/company/synaptrix-ai) and watch the repo for updates.  \n",
      "* Feature your projects by contacting `community@synaptrix.org` or opening a PR.\n",
      "\n",
      "---\n",
      "\n",
      "## Running as an AI User\n",
      "\n",
      "* Use the **Agents Portal**: <https://agents.synaptrix.org/dechat>.  \n",
      "* Import your `.p12` encryption file to interact securely (\"Getting Started\" instructions on the portal).  \n",
      "* The portal prompts you to select the encryption file once you start interacting to secure your AI pipeline.\n",
      "\n",
      "---\n",
      "\n",
      "## FAQ\n",
      "\n",
      "| Question | Answer |\n",
      "|----------|--------|\n",
      "| *Is my data safe?* | All traffic is encrypted with your personal PEM/P12 key. |\n",
      "| *What if I need a different OS?* | We‚Äôre working on macOS and other Linux distros. In the meantime you can use the Docker image to run on different OSes |\n",
      "| *Can I host models on a phone?* | The SDK runs on any machine with Python 3.8‚Äì3.12, but SpiLLIHost currently targets desktop OSes. In principle, yes, but we will postpone our phone hosting efforts unless you have a specific use case you'd like us to support |\n",
      "\n",
      "---\n",
      "\n",
      "**Thank you for using SpiLLI!**  \n",
      "For questions or help, reach out via the GitHub issues or Discussions tab (on the GitHub repo page).\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1cb1e-0c08-4989-a381-f87a4da713f9",
   "metadata": {},
   "source": [
    "### Text Splitting\n",
    "\n",
    "Split the loaded content into smaller manageable chunks of information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ede4e5-14f4-4a82-b61e-abe5b90a7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "documents_web = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e0c77-be2e-4c78-b480-3f7a9316826d",
   "metadata": {},
   "source": [
    "## 1B: Working with Files\n",
    "\n",
    "### Loading PDF Documents\n",
    "\n",
    "For local pdf files, we'll use `PyPDFLoader'\n",
    "\n",
    "For the purpose of this tutorial, lets download an example pdf (A survey on how Agentic RAG systems are built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3acd7f7f-5081-4ab9-a947-6ba2edb3f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/2501.09136\"\n",
    "filename = \"AgenticRAG.pdf\"\n",
    "import requests\n",
    "import os\n",
    "if not os.path.exists(filename):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"PDF downloaded successfully and saved as {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading the PDF: {str(e)}\")\n",
    "        print(f\"Please browse to the url from your browser, download and save it as AgenticRAG.pdf to the scripts folder to use with this tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b8f9d9-43a8-4caf-bab1-a06e42916372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Load content from a PDF file\n",
    "loader = PyPDFLoader(\"AgenticRAG.pdf\")\n",
    "documents_pdf = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a26122-eea9-4782-beb5-bbb5f5b07846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25af43-4d24-402c-9c01-3921ec57c1b0",
   "metadata": {},
   "source": [
    "## 1C: Working with databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d2f18d-d70f-41f9-ad36-a923a7be926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a SQLite database file\n",
    "conn = sqlite3.connect(\"tutorial.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS documents (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    content TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "cursor.executemany(\"INSERT INTO documents (content) VALUES (?)\", [\n",
    "    (\"LangChain makes it easy to work with LLMs.\",),\n",
    "    (\"SQLLoader allows you to load data from SQL databases.\",),\n",
    "    (\"This is a sample document for the tutorial.\",),\n",
    "])\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccd283e-d305-452d-a941-ece9dc119a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.sql_database import SQLDatabaseLoader, SQLDatabase\n",
    "\n",
    "# Example for PostgreSQL\n",
    "db = SQLDatabase.from_uri(\"sqlite:///tutorial.db\")\n",
    "loader = SQLDatabaseLoader(\n",
    "    db=db,\n",
    "    query=\"SELECT content FROM documents;\"\n",
    ")\n",
    "documents_db = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542562b-e123-430f-9427-832a8eab5c37",
   "metadata": {},
   "source": [
    "There are a lot of other data sources that could be of interest to you to retrieve data from. You can find a comprehensive list of document loaders from the community at:\n",
    "\n",
    "https://python.langchain.com/api_reference/community/document_loaders.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c95f4-7d4c-4a49-a657-fa94783ca140",
   "metadata": {},
   "source": [
    "# Step 2: Encoding & Indexing Retrievable Data\n",
    "\n",
    "In order to efficiently search and retrieve the relevant information later, we create an efficient index out of our information pool and save it as a indexed database.\n",
    "\n",
    "We will use a FAISS vector database for creating a efficient index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccefc40-b5a6-4ea1-9c39-1f7c854dfea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2f0688-d791-4f91-aa30-18e417bc271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19889686-1cb5-414f-82df-254cc84ec887",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85150ec-3797-4ad1-8ad6-baff44b1f65f",
   "metadata": {},
   "source": [
    "#### Create a vectorstore for our index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce45dba-0056-4955-932b-67600742b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "vectorstore = FAISS.from_documents(documents_web+documents_pdf, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57d868-7a60-4e6a-b133-2f7fc31deed6",
   "metadata": {},
   "source": [
    "#### Create a retriever object to get indexed documents from the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a2d09a-97f2-4c99-915b-ba3743c734ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1bd6f5-65e0-4815-a61f-16f101946dc0",
   "metadata": {},
   "source": [
    "# Step 3: Use the retriever to Augment LLM context\n",
    "\n",
    "We setup a chain of operation using langchains convenient syntax. The chain performs the following operations for a given query input:\n",
    "\n",
    "1.  first performs a retrieval operation for the relevant documents from the vectordatabase\n",
    "2.  calls the LLM with the user query and retrieved documents passed as an input to the LLM (instead of just the user query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593390d-da8e-451d-9d81-c21088a37c4f",
   "metadata": {},
   "source": [
    "### Lets get an LLM object using SpiLLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88be4258-0c2e-4606-953b-7deb8ffa5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting using cus_id: cus_T42EpGqSccMdhc\n",
      "[Socket] Connected\n",
      "[Socket] Connected\n"
     ]
    }
   ],
   "source": [
    "from SpinLLM import SpinLLM\n",
    "# models to try: llama3.2:latest, gemma3:1b\n",
    "llm = SpinLLM(\n",
    "    model_name= 'Gpt-Oss-20B',#\"llama3.2:latest\",\n",
    "    encryption_path='/root/.spilli/SpiLLI.pem',\n",
    "    temperature=0.8,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031ea8b-7e22-4515-b6fe-90c49d2f0de3",
   "metadata": {},
   "source": [
    "### Create the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e96a2c5-fdf0-479c-a70c-f3f34ad01859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Get the RAG prompt template from LangChain Hub (you can ignore iany langsmithapikeywarnings)\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90d455e7-e77a-4d57-ad5b-b49e190568fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9c0fca6-3df6-445e-bf44-ae9695b4290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_prompt  # You can look at what the prompt look like here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2cb6a-4573-4144-ac1d-e18fc4433013",
   "metadata": {},
   "source": [
    "### Run queries using the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b9ab2ed-091e-4e5d-aae2-e3eebab5b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>We need to answer: \"What can you tell me about SpiLLI?\" Use up to 3 sentences. Provide concise answer. Use context. Provide info: decentralized AI inference infrastructure, host and SDK, beta, etc.<|end|><|start|>assistant<|channel|>final<|message|>SpiLLI is a decentralized AI inference platform that lets machines run as host nodes (SpiLLIHost) and serve AI models to a peer‚Äëto‚Äëpeer network without a central cloud provider. It offers a Python SDK for developers to build applications that discover and use models hosted on any SpiLLI node. The project is currently in beta, supports NVIDIA, AMD GPUs and CPUs, and can be installed on Ubuntu or Windows via the provided installers.[EOG]\n"
     ]
    }
   ],
   "source": [
    "query = \"What can you tell me about SpiLLI?\"\n",
    "response = await qa_chain.ainvoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82d28d7d-3171-445b-a93c-e9f9f9bc5354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>We need to answer: \"What are the principles on which decentralized AI is built?\" Use context: The context includes a snippet: \"*Decentralized AI inference infrastructure* SpiLLI provides infrastructure to manage, host, deploy and run decentra...\" Also other context about agentic workflows: reflection, planning, tool use, multi-agent. But question: \"What are the principles on which decentralized AI is built?\" Likely answer: decentralization, distributed inference, community-driven, privacy, scalability, resilience. Use three sentences maximum. Provide concise answer.<|end|><|start|>assistant<|channel|>final<|message|>Decentralized AI is built on principles of distributed inference, where models run across a network of nodes rather than a single server, enhancing scalability and resilience. It emphasizes community‚Äëdriven governance, allowing users to manage, host, and deploy AI applications through shared infrastructure. Privacy and data sovereignty are also core, ensuring that sensitive information remains local while still benefiting from collective learning.[EOG]\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the principles on which decentralized AI is built?\"\n",
    "response = await qa_chain.ainvoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb84dd-84ce-4197-89b7-ab469b5f1710",
   "metadata": {},
   "source": [
    "#### Naive RAG\n",
    "\n",
    "With the above code we implemented a naive RAG system, where a similarity search is performed directly on the user query to the documents and the closest documents are added to the context of the LLM for generating a response to the query.\n",
    "\n",
    "While this is ok to do in principle, it does not always yield the best results. \n",
    "\n",
    "Why you may ask?  \n",
    "\n",
    "When we perform context retrieval this way, we only look at keywords similarity between keywords in the query and the document objects. But relevance of the document is not just be a matter of searching for keywords, there is often meaning in non keywords, for example \"Tell me about SpiLLI\", yields \"To install SpiLLI SDK, ...\", not quite capturing the intent of what we asked. While it is true that the document relating to installation for SpiLLI SDK (scoringing high on keyword similarity) is relevant, it is not exactly what we asked for. We had to explicitly put the keywords \"decentralized AI\" and \"principles\" in our second query to coax out the document from the retriever similarity search that would have been more meaningful in responding to a query like \"What can you tell me about SpiLLI\". \n",
    "\n",
    "This is a common occurence and problem with Naive RAG and thus more advanced RAG techniques are often used in practice (see the AgenticRAG.pdf for more ideas and details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aff80f2-281e-449c-a8ce-f7a2f3a8fcac",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "LLM stream timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:520\u001b[39m, in \u001b[36mwait_for\u001b[39m\u001b[34m(fut, timeout)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts.timeout(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "\u001b[31mCancelledError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/SpiLLI/SpinLLI.py:50\u001b[39m, in \u001b[36mSpinRes.run\u001b[39m\u001b[34m(self, data, client_id)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait_for(done, timeout=\u001b[32m10.0\u001b[39m)   \u001b[38;5;66;03m# 10‚ÄØs guard\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.TimeoutError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:519\u001b[39m, in \u001b[36mwait_for\u001b[39m\u001b[34m(fut, timeout)\u001b[39m\n\u001b[32m    517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts.timeout(timeout):\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/timeouts.py:115\u001b[39m, in \u001b[36mTimeout.__aexit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task.uncancel() <= \u001b[38;5;28mself\u001b[39m._cancelling \u001b[38;5;129;01mand\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m exceptions.CancelledError:\n\u001b[32m    113\u001b[39m         \u001b[38;5;66;03m# Since there are no new cancel requests, we're\u001b[39;00m\n\u001b[32m    114\u001b[39m         \u001b[38;5;66;03m# handling this.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_val\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01mis\u001b[39;00m _State.ENTERED:\n",
      "\u001b[31mTimeoutError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat can you tell me about Naive RAG in Agentic RAG systems?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m qa_chain.ainvoke(query)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3191\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3189\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3190\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3191\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3192\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:399\u001b[39m, in \u001b[36mBaseLLM.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     **kwargs: Any,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    398\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    400\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    401\u001b[39m         stop=stop,\n\u001b[32m    402\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    403\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    404\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    405\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    407\u001b[39m         **kwargs,\n\u001b[32m    408\u001b[39m     )\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:797\u001b[39m, in \u001b[36mBaseLLM.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    794\u001b[39m     **kwargs: Any,\n\u001b[32m    795\u001b[39m ) -> LLMResult:\n\u001b[32m    796\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    798\u001b[39m         prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    799\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1278\u001b[39m, in \u001b[36mBaseLLM.agenerate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m     run_managers = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1258\u001b[39m         *[\n\u001b[32m   1259\u001b[39m             callback_manager.on_llm_start(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1275\u001b[39m         ]\n\u001b[32m   1276\u001b[39m     )\n\u001b[32m   1277\u001b[39m     run_managers = [r[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m run_managers]  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate_helper(\n\u001b[32m   1279\u001b[39m         prompts,\n\u001b[32m   1280\u001b[39m         stop,\n\u001b[32m   1281\u001b[39m         run_managers,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1282\u001b[39m         new_arg_supported=\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[32m   1283\u001b[39m         **kwargs,\n\u001b[32m   1284\u001b[39m     )\n\u001b[32m   1285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1286\u001b[39m     run_managers = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1287\u001b[39m         *[\n\u001b[32m   1288\u001b[39m             callback_managers[idx].on_llm_start(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1297\u001b[39m         ]\n\u001b[32m   1298\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1087\u001b[39m, in \u001b[36mBaseLLM._agenerate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate_helper\u001b[39m(\n\u001b[32m   1070\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1071\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1076\u001b[39m     **kwargs: Any,\n\u001b[32m   1077\u001b[39m ) -> LLMResult:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1079\u001b[39m         output = (\n\u001b[32m   1080\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1081\u001b[39m                 prompts,\n\u001b[32m   1082\u001b[39m                 stop=stop,\n\u001b[32m   1083\u001b[39m                 run_manager=run_managers[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1084\u001b[39m                 **kwargs,\n\u001b[32m   1085\u001b[39m             )\n\u001b[32m   1086\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(prompts, stop=stop)\n\u001b[32m   1088\u001b[39m         )\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1090\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1091\u001b[39m             *[\n\u001b[32m   1092\u001b[39m                 run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m   1093\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers\n\u001b[32m   1094\u001b[39m             ]\n\u001b[32m   1095\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/Tutorials/Building_RAG_Agents/scripts/SpinLLM.py:42\u001b[39m, in \u001b[36mSpinLLM._agenerate\u001b[39m\u001b[34m(self, prompts, stop, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m generations = []\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     text = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acall(prompt, stop=stop, **kwargs)\n\u001b[32m     43\u001b[39m     gen = Generation(text=text)\n\u001b[32m     44\u001b[39m     generations.append([gen])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/Tutorials/Building_RAG_Agents/scripts/SpinLLM.py:66\u001b[39m, in \u001b[36mSpinLLM._acall\u001b[39m\u001b[34m(self, prompt, stop, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a request to Spin and return the response.\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# response = request_model(\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m#     model=self.model_name,\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m#     prompt=prompt,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m \u001b[38;5;66;03m#     **kwargs\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.run({\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m:prompt})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/SpiLLI/SpinLLI.py:52\u001b[39m, in \u001b[36mSpinRes.run\u001b[39m\u001b[34m(self, data, client_id)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait_for(done, timeout=\u001b[32m10.0\u001b[39m)   \u001b[38;5;66;03m# 10‚ÄØs guard\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.TimeoutError:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLLM stream timed out\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Concatenate everything that was put in the queue\u001b[39;00m\n\u001b[32m     55\u001b[39m result_chunks = []\n",
      "\u001b[31mRuntimeError\u001b[39m: LLM stream timed out"
     ]
    }
   ],
   "source": [
    "query = \"What can you tell me about Naive RAG in Agentic RAG systems?\"\n",
    "response = await qa_chain.ainvoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dabc7ba-5496-40b8-95f3-676c40b84db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, the role of ranking and agents in RAG systems is to enable autonomous decision-making processes, allowing the system to adapt to complex queries and handle diverse data sources.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the role of ranking and agents in RAG systems?\"\n",
    "response = await qa_chain.ainvoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b06cfdb-22b1-4341-9e92-52d6a462ca4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "LLM stream timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:520\u001b[39m, in \u001b[36mwait_for\u001b[39m\u001b[34m(fut, timeout)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts.timeout(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "\u001b[31mCancelledError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/SpiLLI/SpinLLI.py:50\u001b[39m, in \u001b[36mSpinRes.run\u001b[39m\u001b[34m(self, data, client_id)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait_for(done, timeout=\u001b[32m10.0\u001b[39m)   \u001b[38;5;66;03m# 10‚ÄØs guard\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.TimeoutError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:519\u001b[39m, in \u001b[36mwait_for\u001b[39m\u001b[34m(fut, timeout)\u001b[39m\n\u001b[32m    517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts.timeout(timeout):\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/timeouts.py:115\u001b[39m, in \u001b[36mTimeout.__aexit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task.uncancel() <= \u001b[38;5;28mself\u001b[39m._cancelling \u001b[38;5;129;01mand\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m exceptions.CancelledError:\n\u001b[32m    113\u001b[39m         \u001b[38;5;66;03m# Since there are no new cancel requests, we're\u001b[39;00m\n\u001b[32m    114\u001b[39m         \u001b[38;5;66;03m# handling this.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_val\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01mis\u001b[39;00m _State.ENTERED:\n",
      "\u001b[31mTimeoutError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the different RAG strategies?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m qa_chain.ainvoke(query)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3191\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3189\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3190\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3191\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3192\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:399\u001b[39m, in \u001b[36mBaseLLM.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     **kwargs: Any,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    398\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    400\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    401\u001b[39m         stop=stop,\n\u001b[32m    402\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    403\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    404\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    405\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    407\u001b[39m         **kwargs,\n\u001b[32m    408\u001b[39m     )\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:797\u001b[39m, in \u001b[36mBaseLLM.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    794\u001b[39m     **kwargs: Any,\n\u001b[32m    795\u001b[39m ) -> LLMResult:\n\u001b[32m    796\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    798\u001b[39m         prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    799\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1278\u001b[39m, in \u001b[36mBaseLLM.agenerate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m     run_managers = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1258\u001b[39m         *[\n\u001b[32m   1259\u001b[39m             callback_manager.on_llm_start(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1275\u001b[39m         ]\n\u001b[32m   1276\u001b[39m     )\n\u001b[32m   1277\u001b[39m     run_managers = [r[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m run_managers]  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate_helper(\n\u001b[32m   1279\u001b[39m         prompts,\n\u001b[32m   1280\u001b[39m         stop,\n\u001b[32m   1281\u001b[39m         run_managers,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1282\u001b[39m         new_arg_supported=\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[32m   1283\u001b[39m         **kwargs,\n\u001b[32m   1284\u001b[39m     )\n\u001b[32m   1285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1286\u001b[39m     run_managers = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1287\u001b[39m         *[\n\u001b[32m   1288\u001b[39m             callback_managers[idx].on_llm_start(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1297\u001b[39m         ]\n\u001b[32m   1298\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1087\u001b[39m, in \u001b[36mBaseLLM._agenerate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate_helper\u001b[39m(\n\u001b[32m   1070\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1071\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1076\u001b[39m     **kwargs: Any,\n\u001b[32m   1077\u001b[39m ) -> LLMResult:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1079\u001b[39m         output = (\n\u001b[32m   1080\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1081\u001b[39m                 prompts,\n\u001b[32m   1082\u001b[39m                 stop=stop,\n\u001b[32m   1083\u001b[39m                 run_manager=run_managers[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1084\u001b[39m                 **kwargs,\n\u001b[32m   1085\u001b[39m             )\n\u001b[32m   1086\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(prompts, stop=stop)\n\u001b[32m   1088\u001b[39m         )\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1090\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1091\u001b[39m             *[\n\u001b[32m   1092\u001b[39m                 run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m   1093\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers\n\u001b[32m   1094\u001b[39m             ]\n\u001b[32m   1095\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/Tutorials/Building_RAG_Agents/scripts/SpinLLM.py:42\u001b[39m, in \u001b[36mSpinLLM._agenerate\u001b[39m\u001b[34m(self, prompts, stop, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m generations = []\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     text = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acall(prompt, stop=stop, **kwargs)\n\u001b[32m     43\u001b[39m     gen = Generation(text=text)\n\u001b[32m     44\u001b[39m     generations.append([gen])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/Tutorials/Building_RAG_Agents/scripts/SpinLLM.py:66\u001b[39m, in \u001b[36mSpinLLM._acall\u001b[39m\u001b[34m(self, prompt, stop, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a request to Spin and return the response.\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# response = request_model(\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m#     model=self.model_name,\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m#     prompt=prompt,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m \u001b[38;5;66;03m#     **kwargs\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.run({\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m:prompt})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/SpiLLI/SpinLLI.py:52\u001b[39m, in \u001b[36mSpinRes.run\u001b[39m\u001b[34m(self, data, client_id)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait_for(done, timeout=\u001b[32m10.0\u001b[39m)   \u001b[38;5;66;03m# 10‚ÄØs guard\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.TimeoutError:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLLM stream timed out\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Concatenate everything that was put in the queue\u001b[39;00m\n\u001b[32m     55\u001b[39m result_chunks = []\n",
      "\u001b[31mRuntimeError\u001b[39m: LLM stream timed out"
     ]
    }
   ],
   "source": [
    "query = \"What are the different RAG strategies?\"\n",
    "response = await qa_chain.ainvoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f24167-266b-4f68-a160-d130b5ebed62",
   "metadata": {},
   "source": [
    "#### As you may have noticed, the Naive RAG does not have semantic understanding of the documents and user queries and thus often fails to retrieve the relevant information even when present in the vector database. This is where addtional LLM and agentic steps are required to improve the retrieval performance.\n",
    "\n",
    "#### We can improve upon the retrieval performance using ideas like \"Re-ranking\", \"Chain-of-thought\" and agentic retrieval where AI models are used to in a multi-step retrieval process to create a better set of retrieved documents to feed into the final response generation LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d1c03-5f72-4994-9290-8a90bcc3abe8",
   "metadata": {},
   "source": [
    "# Step 4: Performance improvement\n",
    "\n",
    "## Re-ranking Models for RAG\n",
    "\n",
    "### Exercise: Implement document re-ranking as a intermediate step between retrieval and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42ecd9-b99b-4f7b-b354-d6011ca80805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bac45116-2514-4145-b40f-eda53b6cd6a0",
   "metadata": {},
   "source": [
    "## Chain-of-Thought for RAG\n",
    "\n",
    "### Exercise: Implement a a multi-step chain of thought for response generation from the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb960e3-cefa-466a-82a8-a75d26c1a084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dce048-e391-488f-ae22-b9460c5dd364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpiLLI",
   "language": "python",
   "name": "spilli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

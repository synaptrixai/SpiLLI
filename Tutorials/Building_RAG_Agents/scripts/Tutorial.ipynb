{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e300a830-db9d-4cd2-84a6-57b06a7afaae",
   "metadata": {},
   "source": [
    "# Step 1: Retrieving Information from different sources (Websites, Files, Databases ...)\n",
    "\n",
    "## 1A: Loading Web Content \n",
    "\n",
    "We'll use LangChain's Selenium plugin from the Unstructured library to retrieve content from websites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c57acf0-afe0-4f44-83f3-eda47e7a8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import SeleniumURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22371cc-6ea7-4233-9d18-1c8bcad38106",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://raw.githubusercontent.com/synaptrixai/SpiLLI/refs/heads/main/README.md\"]\n",
    "loader = SeleniumURLLoader(urls=urls, browser='chrome', headless=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91df3ee8-eac3-42b2-a93d-484059f32019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SpiLLI\n",
      "SpiLLI provides infrastructure to manage, host, deploy and run decentralized AI inference\n",
      "\n",
      "SpiLLI infrastructure comprises of two components: 1. SpiLLI SDK (a library / framework to write decentralized AI applications) and 2. SpiLLIHost (a host software allowing decentralized nodes to execute and connect AI models to peer nodes)\n",
      "\n",
      "**Note SpiLLI is currently in beta testing and comes with no warranties, can have bugs and we appreciate you helping us iron out all its flaws with feedback and suggestions in the Issues and Discussions tabs in this repository**\n",
      "\n",
      "# System requirements\n",
      "\n",
      "We currently support Ubuntu 24.04 and Windows 10/11 operating systems for **SpiLLIHost** (host nodes for AI models). SpiLLIHost currently requires a NVidia GPU (with its driver installed) to run the AI models. Support for other OS and GPU/CPU variants will be coming soon.\n",
      "\n",
      "SpiLLI SDK currently provides an interface to python 3.12 (support for other python versions and languages coming soon).\n",
      "\n",
      "# Installation\n",
      "\n",
      "## SpiLLIHost\n",
      "\n",
      "You can download and install SpiLLIHost for Ubuntu and Windows on computers with a NVidia GPU by downloading the corresponding installers from our current mirrors on SourceForge using the buttons / links below:\n",
      "\n",
      "## For Ubuntu:\n",
      "\n",
      "Download button  => \n",
      "<a href=\"https://sourceforge.net/projects/spilli/files/V0.2.7/SpiLLIHost-0.2.7-Linux-SpiLLIHost.deb/download\" target=\"_blank\">\n",
      "  <img src=\"https://a.fsdn.com/con/app/sf-download-button\" alt=\"Download SpiLLIHost\"/>\n",
      "</a>\n",
      "\n",
      "Link: https://sourceforge.net/projects/spilli/files/V0.2.7/SpiLLIHost-0.2.7-Linux-SpiLLIHost.deb/download\n",
      "\n",
      "After the installer (.deb file) is downloaded, to install SpiLLIHost on ubuntu, open a terminal in the directory containing the downloaded file and install using Ubuntu's \"apt\" package manager\n",
      "\n",
      "```\n",
      "sudo apt install ./SpiLLIHost-0.2.7-Linux-SpiLLIHost.deb\n",
      "```\n",
      "\n",
      "SpiLLIHost uses a personalized encryption key given by a personalized SpiLLIHost.pem. Your host node cannot function without this key. To make the host usable, download the SpiLLIHost Encryption file from https://agents.syanptrix.org/dechat (Click on the download SpiLLIHost button on the page and click on the \"Download Host Encryption\" button). \n",
      "\n",
      "```\n",
      "sudo mv ./SpiLLIHost.pem /usr/bin/SpiLLIHost/\n",
      "```\n",
      "\n",
      "With the encryption file now placed in /usr/bin/SpiLLIHost/ your host node becomes functional. The encryption key is used to secure communication between the host and peer nodes, and also is used to keep track of usage of host nodes on the network to allow host nodes to be compensated with credits in the future (more on this later below).\n",
      "\n",
      "## For Windows:\n",
      "\n",
      "Download button => \n",
      "<a href=\"https://sourceforge.net/projects/spilli/files/V0.2.7/SpiLLIHost-0.2.7-win64.exe/download\" target=\"_blank\">\n",
      "  <img src=\"https://a.fsdn.com/con/app/sf-download-button\" alt=\"Download SpiLLIHost\"/>\n",
      "</a>\n",
      "\n",
      "Link: https://sourceforge.net/projects/spilli/files/V0.2.7/SpiLLIHost-0.2.7-win64.exe/download\n",
      "\n",
      "To install, double click on the installer to run the setup. (The installer is not currently verfied with a windows signature and windows will warn you about this. You can ignore the warning and proceed with running the setup. This is not indicative of any malware, just requires us to register the installer with a digital signature. The fix for this will be coming soon).\n",
      "\n",
      "## SpiLLI SDK\n",
      "\n",
      "If you have python 3.12 and pip installed you can install the SDK simply by running\n",
      "\n",
      "```\n",
      "pip install --index-url https://tech.synaptrix.org/pypi/ --client-cert ./SpiLLI.pem --upgrade SpiLLI\n",
      "```\n",
      "\n",
      "If you need to install python, you can follow any of the guides on the internet for your operating system. Just make sure that you are installing python version 3.12 as this is the only one currently supported (support for other versions coming soon).\n",
      "\n",
      "The SpiLLI SDK also uses a SpiLLI.pem encryption file similar to the one used for SpiLLIHost above and can be downloaded from  https://agents.synaptrix.org/dechat by clicking the download SpiLLI SDK button and then click download SDK encryption.\n",
      "\n",
      "## How does SpiLLI work?\n",
      "\n",
      "SpiLLI is a decentralized network of AI hosts and AI users. When you download and install SpiLLIHost on a computer, the computer becomes a host node on the network. Other computers (not excluding the host computer) can run applications built with SpiLLI SDK that can use AI models hosted on any of the host nodes in the network. \n",
      "\n",
      "SpiLLI allows applications to connect directly to the best available computing resources, without relying on centralized cloud providers.\n",
      "\n",
      "The network works through **three key principles**:\n",
      "\n",
      "1. **Dynamic Resource Allocation**: Applications automatically discover and connect to the best available hosts based on current demand, performance, and proximity\n",
      "\n",
      "2. **Decentralized Infrastructure**: Computing resources are distributed across a global network of participating hosts, creating redundancy and fault tolerance\n",
      "\n",
      "3. **Real-time Adaptation**: The network continuously optimizes connections and resource allocation as conditions change, ensuring optimal performance\n",
      "\n",
      "This approach reduces costs for developers while making it easier for anyone to contribute computing resources to power AI applications.\n",
      "\n",
      "The approach makes AI development more affordable and accessible, enabling anyone to contribute computing resources and developers to rely on a robust, decentralized network instead of centralized cloud services.\n",
      "\n",
      "## How can you contribute?\n",
      "\n",
      "### 1. üöÄ Become an AI Host\n",
      "- Download **SpiLLIHost** and join our network to host AI models for the community.\n",
      "- Currently in beta testing, earn tokens (no monetary value) while hosting resources.\n",
      "- Early hosts will be recognized as champions and supporters of the project.\n",
      "\n",
      "### 2. üíª Develop Decentralized AI Apps\n",
      "- Build innovative apps using our getting started guides and tutorials available in the Wiki.\n",
      "- Connect your apps to our community network of hosts for global access without high costs.\n",
      "- Share your ideas, examples, and projects with the community to get featured!\n",
      "\n",
      "### 3. üí° Give Feedback & Ideas\n",
      "- Let us know if your run into any issues or suggest features.\n",
      "- Share testimonials about how SpiLLI is helpful for you or your team.\n",
      "- Don‚Äôt forget to star this repository if you like what we‚Äôre building! üëç\n",
      "\n",
      "### 4. üèÜ Become a Sponsor\n",
      "- Support our mission by sponsoring community events, hackathons, or development efforts.\n",
      "- Get featured as a supporter in our repository and community resources.\n",
      "- Contact us at [community@synaptrix.org](mailto:community@synaptrix.org) to learn more about sponsorship opportunities.\n",
      "\n",
      "\n",
      "# Getting Started\n",
      "\n",
      "## As an AI Host\n",
      "\n",
      "### Installation\n",
      "Download and install the SpiLLIHost app from the button above. The installation will automatically set up a host service on your computer.\n",
      "\n",
      "### Managing Services\n",
      "- **Windows:** Control the start/stop of the host service through Services settings.\n",
      "- **Ubuntu:** Use `systemctl` commands to manage the service.\n",
      "\n",
      "The host service typically runs by default after installation.\n",
      "\n",
      "### Hosting Models\n",
      "1. Visit [Agents Portal](https://agents.synaptrix.org/dechat/) and use the \"Manage Hosted Models\" button to:\n",
      "   - View your host nodes\n",
      "   - Add/remove models\n",
      "2. Alternatively, you can directly place a `.gguf` model files in the `Models` directory located at:\n",
      "   - **Ubuntu:** `/usr/bin/SpiLLIHost`\n",
      "   - **Windows:** `C:/Program Files (x86)/SpiLLIHost`\n",
      "\n",
      "## As an AI Developer\n",
      "Explore our [Tutorials](Tutorials) folder for example applications and ideas to help you develop your own projects. You can add this repository to your watch list or follow us on [LinkedIn](https://www.linkedin.com/company/synaptrix-ai). We will regularly update you via the Discussions section in the repository and via posts on LinkedIn on new tutorials, implementations and capability updates as the project advances. We will also be happy to feature your projects and ideas built using SpiLLI on these pages, just contact us with a draft on community@synaptrix.org or send us a pull request on this repository. \n",
      "\n",
      "## As a General AI User\n",
      "\n",
      "You can explore AI models and chat with models on the decentralized network using the [Agents Portal](https://agents.synaptrix.org/dechat/). Note you'll need to setup your encryption by importing a .p12 encryption file to interact with the models on the Agents Portal. The encryption protects your data over the network such that only you have access to your data. The steps to getting and importing the encryption file to your browser are shown in the Getting Started section on the Agents Portal. Your browser will prompt you to select the encryption file to use when interactive with the portal after it has been setup. \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1cb1e-0c08-4989-a381-f87a4da713f9",
   "metadata": {},
   "source": [
    "### Text Splitting\n",
    "\n",
    "Split the loaded content into smaller manageable chunks of information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ede4e5-14f4-4a82-b61e-abe5b90a7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "documents_web = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e0c77-be2e-4c78-b480-3f7a9316826d",
   "metadata": {},
   "source": [
    "## 1B: Working with Files\n",
    "\n",
    "### Loading PDF Documents\n",
    "\n",
    "For local pdf files, we'll use `PyPDFLoader'\n",
    "\n",
    "For the purpose of this tutorial, lets download an example pdf (A survey on how Agentic RAG systems are built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3acd7f7f-5081-4ab9-a947-6ba2edb3f8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded successfully and saved as AgenticRAG.pdf\n"
     ]
    }
   ],
   "source": [
    "url = \"https://arxiv.org/pdf/2501.09136\"\n",
    "filename = \"AgenticRAG.pdf\"\n",
    "import requests\n",
    "import os\n",
    "if not os.path.exists(filename):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"PDF downloaded successfully and saved as {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading the PDF: {str(e)}\")\n",
    "        print(f\"Please browse to the url from your browser, download and save it as AgenticRAG.pdf to the scripts folder to use with this tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b8f9d9-43a8-4caf-bab1-a06e42916372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "# Load content from a PDF file\n",
    "loader = PyPDFLoader(\"AgenticRAG.pdf\")\n",
    "documents_pdf = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a26122-eea9-4782-beb5-bbb5f5b07846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25af43-4d24-402c-9c01-3921ec57c1b0",
   "metadata": {},
   "source": [
    "## 1C: Working with databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d2f18d-d70f-41f9-ad36-a923a7be926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a SQLite database file\n",
    "conn = sqlite3.connect(\"tutorial.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS documents (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    content TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "cursor.executemany(\"INSERT INTO documents (content) VALUES (?)\", [\n",
    "    (\"LangChain makes it easy to work with LLMs.\",),\n",
    "    (\"SQLLoader allows you to load data from SQL databases.\",),\n",
    "    (\"This is a sample document for the tutorial.\",),\n",
    "])\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccd283e-d305-452d-a941-ece9dc119a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.sql_database import SQLDatabaseLoader, SQLDatabase\n",
    "\n",
    "# Example for PostgreSQL\n",
    "db = SQLDatabase.from_uri(\"sqlite:///tutorial.db\")\n",
    "loader = SQLDatabaseLoader(\n",
    "    db=db,\n",
    "    query=\"SELECT content FROM documents;\"\n",
    ")\n",
    "documents_db = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542562b-e123-430f-9427-832a8eab5c37",
   "metadata": {},
   "source": [
    "There are a lot of other data sources that could be of interest to you to retrieve data from. You can find a comprehensive list of document loaders from the community at:\n",
    "\n",
    "https://python.langchain.com/api_reference/community/document_loaders.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c95f4-7d4c-4a49-a657-fa94783ca140",
   "metadata": {},
   "source": [
    "# Step 2: Encoding & Indexing Retrievable Data\n",
    "\n",
    "In order to efficiently search and retrieve the relevant information later, we create an efficient index out of our information pool and save it as a indexed database.\n",
    "\n",
    "We will use a FAISS vector database for creating a efficient index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccefc40-b5a6-4ea1-9c39-1f7c854dfea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2f0688-d791-4f91-aa30-18e417bc271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19889686-1cb5-414f-82df-254cc84ec887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85150ec-3797-4ad1-8ad6-baff44b1f65f",
   "metadata": {},
   "source": [
    "#### Create a vectorstore for our index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce45dba-0056-4955-932b-67600742b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "vectorstore = FAISS.from_documents(documents_web+documents_pdf, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57d868-7a60-4e6a-b133-2f7fc31deed6",
   "metadata": {},
   "source": [
    "#### Create a retriever object to get indexed documents from the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a2d09a-97f2-4c99-915b-ba3743c734ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1bd6f5-65e0-4815-a61f-16f101946dc0",
   "metadata": {},
   "source": [
    "# Step 3: Use the retriever to Augment LLM context\n",
    "\n",
    "We setup a chain of operation using langchains convenient syntax. The chain performs the following operations for a given query input:\n",
    "\n",
    "1.  first performs a retrieval operation for the relevant documents from the vectordatabase\n",
    "2.  calls the LLM with the user query and retrieved documents passed as an input to the LLM (instead of just the user query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593390d-da8e-451d-9d81-c21088a37c4f",
   "metadata": {},
   "source": [
    "### Lets get an LLM object using SpiLLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88be4258-0c2e-4606-953b-7deb8ffa5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting using cus_id: cus_RYZvwKJNJ4M6Jc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid message format: Missing command field\n"
     ]
    }
   ],
   "source": [
    "from SpinLLM import SpinLLM\n",
    "# models to try: llama3.2:latest, gemma3:1b\n",
    "llm = SpinLLM(\n",
    "    model_name=\"llama3.2:latest\",\n",
    "    encryption_path='/root/.spilli/SpiLLI.pem',\n",
    "    temperature=0.8,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031ea8b-7e22-4515-b6fe-90c49d2f0de3",
   "metadata": {},
   "source": [
    "### Create the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e96a2c5-fdf0-479c-a70c-f3f34ad01859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Get the RAG prompt template from LangChain Hub (you can ignore iany langsmithapikeywarnings)\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90d455e7-e77a-4d57-ad5b-b49e190568fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9c0fca6-3df6-445e-bf44-ae9695b4290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_prompt  # You can look at what the prompt look like here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2cb6a-4573-4144-ac1d-e18fc4433013",
   "metadata": {},
   "source": [
    "### Run queries using the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b9ab2ed-091e-4e5d-aae2-e3eebab5b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpiLLI is a decentralized network of AI hosts and AI users that allows applications to connect to the best available computing resources without relying on centralized cloud providers. It uses three key principles: dynamic resource allocation, decentralized nodes, and AI models. SpiLLI is currently in beta testing and has no warranties, with potential bugs and limitations.\n"
     ]
    }
   ],
   "source": [
    "query = \"What can you tell me about SpiLLI?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d28d7d-3171-445b-a93c-e9f9f9bc5354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decentralized AI is built on three key principles: Dynamic Resource Allocation, Decentralized Infrastructure, and Real-time Adaptation.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the principles on which decentralized AI is built?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb84dd-84ce-4197-89b7-ab469b5f1710",
   "metadata": {},
   "source": [
    "#### Naive RAG\n",
    "\n",
    "With the above code we implemented a naive RAG system, where a similarity search is performed directly on the user query to the documents and the closest documents are added to the context of the LLM for generating a response to the query.\n",
    "\n",
    "While this is ok to do in principle, it does not always yield the best results. \n",
    "\n",
    "Why you may ask?  \n",
    "\n",
    "When we perform context retrieval this way, we only look at keywords similarity between keywords in the query and the document objects. But relevance of the document is not just be a matter of searching for keywords, there is often meaning in non keywords, for example \"Tell me about SpiLLI\", yields \"To install SpiLLI SDK, ...\", not quite capturing the intent of what we asked. While it is true that the document relating to installation for SpiLLI SDK (scoringing high on keyword similarity) is relevant, it is not exactly what we asked for. We had to explicitly put the keywords \"decentralized AI\" and \"principles\" in our second query to coax out the document from the retriever similarity search that would have been more meaningful in responding to a query like \"What can you tell me about SpiLLI\". \n",
    "\n",
    "This is a common occurence and problem with Naive RAG and thus more advanced RAG techniques are often used in practice (see the AgenticRAG.pdf for more ideas and details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aff80f2-281e-449c-a8ce-f7a2f3a8fcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"What can you tell me about Naive RAG in Agentic RAG systems?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dabc7ba-5496-40b8-95f3-676c40b84db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, the role of ranking and agents in RAG systems is to enable autonomous decision-making processes, allowing the system to adapt to complex queries and handle diverse data sources.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the role of ranking and agents in RAG systems?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b06cfdb-22b1-4341-9e92-52d6a462ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the different RAG strategies?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f24167-266b-4f68-a160-d130b5ebed62",
   "metadata": {},
   "source": [
    "#### As you may have noticed, the Naive RAG does not have semantic understanding of the documents and user queries and thus often fails to retrieve the relevant information even when present in the vector database. This is where addtional LLM and agentic steps are required to improve the retrieval performance.\n",
    "\n",
    "#### We can improve upon the retrieval performance using ideas like \"Re-ranking\", \"Chain-of-thought\" and agentic retrieval where AI models are used to in a multi-step retrieval process to create a better set of retrieved documents to feed into the final response generation LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d1c03-5f72-4994-9290-8a90bcc3abe8",
   "metadata": {},
   "source": [
    "# Step 4: Performance improvement\n",
    "\n",
    "## Re-ranking Models for RAG\n",
    "\n",
    "### Exercise: Implement document re-ranking as a intermediate step between retrieval and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42ecd9-b99b-4f7b-b354-d6011ca80805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bac45116-2514-4145-b40f-eda53b6cd6a0",
   "metadata": {},
   "source": [
    "## Chain-of-Thought for RAG\n",
    "\n",
    "### Exercise: Implement a a multi-step chain of thought for response generation from the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb960e3-cefa-466a-82a8-a75d26c1a084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dce048-e391-488f-ae22-b9460c5dd364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpiLLI",
   "language": "python",
   "name": "spilli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
